{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import BinaryIO, Iterator, Iterable\n",
    "import pandas as pd\n",
    "import traceback\n",
    "import zstandard\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "import tqdm\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"dndmaps\"\n",
    "fileOrFolderPath = f'../data/posts/r_{subreddit}_posts.jsonl'\n",
    "output_dir = f\"../data/imgs/{subreddit}\"\n",
    "\n",
    "# Imgs downloading config:\n",
    "download_images = False\n",
    "compress_images = False\n",
    "compress_quality = 70\n",
    "\n",
    "# Make dir if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Add captions to imgs\n",
    "add_captions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTime(seconds: float) -> str:\n",
    "\tif seconds == 0:\n",
    "\t\treturn \"0s\"\n",
    "\tif seconds < 0.001:\n",
    "\t\treturn f\"{seconds * 1_000_000:.1f}µs\"\n",
    "\tif seconds < 1:\n",
    "\t\treturn f\"{seconds * 1_000:.2f}ms\"\n",
    "\telapsedHr = int(seconds // 3600)\n",
    "\telapsedMin = int((seconds % 3600) // 60)\n",
    "\telapsedSec = int(seconds % 60)\n",
    "\treturn f\"{elapsedHr:02}:{elapsedMin:02}:{elapsedSec:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProgressLog:\n",
    "\tfile: BinaryIO\n",
    "\tfileSize: int\n",
    "\ti: int\n",
    "\tstartTime: float\n",
    "\tprintEvery: int\n",
    "\tmaxLineLength: int\n",
    "\n",
    "\tdef __init__(self, path: str, file: BinaryIO):\n",
    "\t\tself.file = file\n",
    "\t\tself.fileSize = os.path.getsize(path)\n",
    "\t\tself.i = 0\n",
    "\t\tself.startTime = time.time()\n",
    "\t\tself.printEvery = 10_000\n",
    "\t\tself.maxLineLength = 0\n",
    "\t\n",
    "\tdef onRow(self):\n",
    "\t\tself.i += 1\n",
    "\t\tif self.i % self.printEvery == 0 and self.i > 0:\n",
    "\t\t\tself.logProgress()\n",
    "\t\t\n",
    "\tdef logProgress(self, end=\"\"):\n",
    "\t\tprogress = self.file.tell() / self.fileSize if not self.file.closed else 1\n",
    "\t\telapsed = time.time() - self.startTime\n",
    "\t\tremaining = (elapsed / progress - elapsed) if progress > 0 else 0\n",
    "\t\ttimePerRow = elapsed / self.i\n",
    "\t\tprintStr = f\"{self.i:,} - {progress:.2%} - elapsed: {formatTime(elapsed)} - remaining: {formatTime(remaining)} - {formatTime(timePerRow)}/row\"\n",
    "\t\tself.maxLineLength = max(self.maxLineLength, len(printStr))\n",
    "\t\tprintStr = printStr.ljust(self.maxLineLength)\n",
    "\t\tprint(f\"\\r{printStr}\", end=end)\n",
    "\n",
    "\t\tif timePerRow < 20/1000/1000:\n",
    "\t\t\tself.printEvery = 20_000\n",
    "\t\telif timePerRow < 50/1000/1000:\n",
    "\t\t\tself.printEvery = 10_000\n",
    "\t\telse:\n",
    "\t\t\tself.printEvery = 5_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "\timport orjson as json\n",
    "except ImportError:\n",
    "\timport json\n",
    "\tprint(\"Recommended to install 'orjson' for faster JSON parsing\")\n",
    "\n",
    "def getZstFileJsonStream(f: BinaryIO, chunk_size=1024*1024*10) -> Iterator[dict]:\n",
    "\tdecompressor = zstandard.ZstdDecompressor(max_window_size=2**31)\n",
    "\tcurrentString = \"\"\n",
    "\tdef yieldLinesJson():\n",
    "\t\tnonlocal currentString\n",
    "\t\tlines = currentString.split(\"\\n\")\n",
    "\t\tcurrentString = lines[-1]\n",
    "\t\tfor line in lines[:-1]:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tyield json.loads(line)\n",
    "\t\t\texcept json.JSONDecodeError:\n",
    "\t\t\t\tprint(\"Error parsing line: \" + line)\n",
    "\t\t\t\ttraceback.print_exc()\n",
    "\t\t\t\tcontinue\n",
    "\tzstReader = decompressor.stream_reader(f)\n",
    "\twhile True:\n",
    "\t\ttry:\n",
    "\t\t\tchunk = zstReader.read(chunk_size)\n",
    "\t\texcept zstandard.ZstdError:\n",
    "\t\t\tprint(\"Error reading zst chunk\")\n",
    "\t\t\ttraceback.print_exc()\n",
    "\t\t\tbreak\n",
    "\t\tif not chunk:\n",
    "\t\t\tbreak\n",
    "\t\tcurrentString += chunk.decode(\"utf-8\", \"replace\")\n",
    "\t\t\n",
    "\t\tyield from yieldLinesJson()\n",
    "\t\n",
    "\tyield from yieldLinesJson()\n",
    "\t\n",
    "\tif len(currentString) > 0:\n",
    "\t\ttry:\n",
    "\t\t\tyield json.loads(currentString)\n",
    "\t\texcept json.JSONDecodeError:\n",
    "\t\t\tprint(\"Error parsing line: \" + currentString)\n",
    "\t\t\tprint(traceback.format_exc())\n",
    "\t\t\tpass\n",
    "\n",
    "def getJsonLinesFileJsonStream(f: BinaryIO) -> Iterator[dict]:\n",
    "\tfor line in f:\n",
    "\t\tline = line.decode(\"utf-8\", errors=\"replace\")\n",
    "\t\ttry:\n",
    "\t\t\tyield json.loads(line)\n",
    "\t\texcept json.JSONDecodeError:\n",
    "\t\t\tprint(\"Error parsing line: \" + line)\n",
    "\t\t\ttraceback.print_exc()\n",
    "\t\t\tcontinue\n",
    "\n",
    "def getFileJsonStream(path: str, f: BinaryIO) -> Iterator[dict]|None:\n",
    "\tif path.endswith(\".jsonl\"):\n",
    "\t\treturn getJsonLinesFileJsonStream(f)\n",
    "\telif path.endswith(\".zst\"):\n",
    "\t\treturn getZstFileJsonStream(f)\n",
    "\telse:\n",
    "\t\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file ../data/posts/r_dndmaps_posts.jsonl\n",
      "52,215 - 100.00% - elapsed: 772.99ms - remaining: 0s - 14.8µs/row     \n",
      "Done :>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Permalink</th>\n",
       "      <th>Id</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>User</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>NoLikes</th>\n",
       "      <th>NoReplies</th>\n",
       "      <th>ImagesUrls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/r/dndmaps/comments/5qfcd8/denham_village/</td>\n",
       "      <td>5qfcd8</td>\n",
       "      <td>dndmaps</td>\n",
       "      <td>hornbook1776</td>\n",
       "      <td>Denham Village</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-27 06:08:04</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>[https://i.redditmedia.com/OABq21shxshcdZzNg8S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/r/dndmaps/comments/5qfiym/another_village_win...</td>\n",
       "      <td>5qfiym</td>\n",
       "      <td>dndmaps</td>\n",
       "      <td>hornbook1776</td>\n",
       "      <td>Another Village - Winshire</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-27 06:52:23</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>[https://i.redditmedia.com/r7XAKScrtgZ7rfEfcFe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/r/dndmaps/comments/5qfm0a/unlabeled_home_or_s...</td>\n",
       "      <td>5qfm0a</td>\n",
       "      <td>dndmaps</td>\n",
       "      <td>nuggsgalore</td>\n",
       "      <td>Unlabeled Home or Safe House</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-27 07:14:34</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>[https://i.redditmedia.com/kDjA0yiANC1mgd2IUPM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/r/dndmaps/comments/5qg8sq/map_of_tear_from_wh...</td>\n",
       "      <td>5qg8sq</td>\n",
       "      <td>dndmaps</td>\n",
       "      <td>NikoRaito</td>\n",
       "      <td>Map of Tear from Wheel of Time with removed la...</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-27 10:30:30</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>[https://i.redditmedia.com/4-t6gwQX9gsviECzwEp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/r/dndmaps/comments/5qgrm1/region_map_the_cont...</td>\n",
       "      <td>5qgrm1</td>\n",
       "      <td>dndmaps</td>\n",
       "      <td>leaderproxima</td>\n",
       "      <td>[Region Map] The Continent of Ethildaan</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-27 13:11:24</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[https://i.redditmedia.com/MksE3bkT-HUF7mU3EWG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Permalink      Id Subreddit  \\\n",
       "0         /r/dndmaps/comments/5qfcd8/denham_village/  5qfcd8   dndmaps   \n",
       "1  /r/dndmaps/comments/5qfiym/another_village_win...  5qfiym   dndmaps   \n",
       "2  /r/dndmaps/comments/5qfm0a/unlabeled_home_or_s...  5qfm0a   dndmaps   \n",
       "3  /r/dndmaps/comments/5qg8sq/map_of_tear_from_wh...  5qg8sq   dndmaps   \n",
       "4  /r/dndmaps/comments/5qgrm1/region_map_the_cont...  5qgrm1   dndmaps   \n",
       "\n",
       "            User                                              Title Content  \\\n",
       "0   hornbook1776                                     Denham Village           \n",
       "1   hornbook1776                         Another Village - Winshire           \n",
       "2    nuggsgalore                       Unlabeled Home or Safe House           \n",
       "3      NikoRaito  Map of Tear from Wheel of Time with removed la...           \n",
       "4  leaderproxima            [Region Map] The Continent of Ethildaan           \n",
       "\n",
       "             Timestamp  NoLikes  NoReplies  \\\n",
       "0  2017-01-27 06:08:04       43          8   \n",
       "1  2017-01-27 06:52:23       35          1   \n",
       "2  2017-01-27 07:14:34       43          2   \n",
       "3  2017-01-27 10:30:30       68          9   \n",
       "4  2017-01-27 13:11:24       16          1   \n",
       "\n",
       "                                          ImagesUrls  \n",
       "0  [https://i.redditmedia.com/OABq21shxshcdZzNg8S...  \n",
       "1  [https://i.redditmedia.com/r7XAKScrtgZ7rfEfcFe...  \n",
       "2  [https://i.redditmedia.com/kDjA0yiANC1mgd2IUPM...  \n",
       "3  [https://i.redditmedia.com/4-t6gwQX9gsviECzwEp...  \n",
       "4  [https://i.redditmedia.com/MksE3bkT-HUF7mU3EWG...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "version = sys.version_info\n",
    "if version.major < 3 or (version.major == 3 and version.minor < 10):\n",
    "\traise RuntimeError(\"This script requires Python 3.10 or higher\")\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "recursive = False\n",
    "\n",
    "\n",
    "def processFile(path: str):\n",
    "\tprint(f\"Processing file {path}\")\n",
    "\tpost_data = []\n",
    "\twith open(path, \"rb\") as f:\n",
    "\t\tjsonStream = getFileJsonStream(path, f)\n",
    "\t\tif jsonStream is None:\n",
    "\t\t\tprint(f\"Skipping unknown file {path}\")\n",
    "\t\t\treturn\n",
    "\t\tprogressLog = FileProgressLog(path, f)\n",
    "\t\tfor row in jsonStream:\n",
    "\t\t\tprogressLog.onRow()\n",
    "\t\t\t\n",
    "\t\t\t# Permalink, Id, Subreddit, User, Type, Title, Content, Timestamp, NoLikes, NoReplies, ImagesUrls\n",
    "\t\t\t\n",
    "\t\t\tpermalink = row[\"permalink\"]\n",
    "\t\t\tid = row[\"id\"]\n",
    "\t\t\tsubreddit = row[\"subreddit\"]\n",
    "\t\t\tuser = row[\"author\"]\n",
    "\t\t\ttitle = row[\"title\"]\n",
    "\t\t\tcontent = row[\"selftext\"]\n",
    "\t\t\ttimestamp = datetime.datetime.fromtimestamp(row.get(\"created_utc\", 0)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\t\t\tscore = row[\"score\"]\n",
    "\t\t\treplies = row[\"num_comments\"]\n",
    "\t\t\t\n",
    "\t\t\timages_urls = []\n",
    "\t\t\tmedia_metadata = row.get('media_metadata')\n",
    "\t\t\tif isinstance(media_metadata, dict):\n",
    "\t\t\t\tfor img in media_metadata.values():\n",
    "\t\t\t\t\tif isinstance(img, dict):\n",
    "\t\t\t\t\t\tif img.get('e') == 'Image':\n",
    "\t\t\t\t\t\t\ts = img.get('s')\n",
    "\t\t\t\t\t\t\tif isinstance(s, dict):\n",
    "\t\t\t\t\t\t\t\turl = s.get('u')\n",
    "\t\t\t\t\t\t\t\tif url:\n",
    "\t\t\t\t\t\t\t\t\timages_urls.append(url.replace(\"&amp;\", \"&\"))\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tpass\n",
    "\t\t\telif isinstance(row.get('preview'), dict):\n",
    "\t\t\t\timages = row['preview'].get('images', [])\n",
    "\t\t\t\tfor image in images:\n",
    "\t\t\t\t\tsource = image.get('source', {})\n",
    "\t\t\t\t\turl = source.get('url')\n",
    "\t\t\t\t\tif url:\n",
    "\t\t\t\t\t\timages_urls.append(url.replace(\"&amp;\", \"&\"))\n",
    "\t\t\telif 'url' in row and row.get('post_hint') == 'image':\n",
    "\t\t\t\timages_urls = [row['url']]\n",
    "\n",
    "\t\t\tif not images_urls:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tpost_data.append({\n",
    "\t\t\t\t\"Permalink\": permalink,\n",
    "\t\t\t\t\"Id\": id,\n",
    "\t\t\t\t\"Subreddit\": subreddit,\n",
    "\t\t\t\t\"User\": user,\n",
    "\t\t\t\t\"Title\": title,\n",
    "\t\t\t\t\"Content\": content,\n",
    "\t\t\t\t\"Timestamp\": timestamp,\n",
    "\t\t\t\t\"NoLikes\": score,\n",
    "\t\t\t\t\"NoReplies\": replies,\n",
    "\t\t\t\t\"ImagesUrls\": images_urls,\n",
    "\t\t\t})\n",
    "\n",
    "\t\t\t# print(f\"Link: {permalink} - Id: {id} - r/: {subreddit} - User: {user} - Type: {type} - Title: {title} - Content: {content} - Time: {timestamp} - Score: {score} - Replies: {replies} - ImagesUrls: {images_urls}\")\n",
    "\n",
    "\t\tprogressLog.logProgress(\"\\n\")\n",
    "\t\tdf = pd.DataFrame(post_data)\n",
    "\t\treturn df\n",
    "\t\n",
    "\n",
    "def processFolder(path: str):\n",
    "\tfileIterator: Iterable[str]\n",
    "\tif recursive:\n",
    "\t\tdef recursiveFileIterator():\n",
    "\t\t\tfor root, dirs, files in os.walk(path):\n",
    "\t\t\t\tfor file in files:\n",
    "\t\t\t\t\tyield os.path.join(root, file)\n",
    "\t\tfileIterator = recursiveFileIterator()\n",
    "\telse:\n",
    "\t\tfileIterator = os.listdir(path)\n",
    "\t\tfileIterator = (os.path.join(path, file) for file in fileIterator)\n",
    "\t\n",
    "\tfor i, file in enumerate(fileIterator):\n",
    "\t\tprint(f\"Processing file {i+1: 3} {file}\")\n",
    "\t\tprocessFile(file)\n",
    "\n",
    "if os.path.isdir(fileOrFolderPath):\n",
    "\tprocessFolder(fileOrFolderPath)\n",
    "else:\n",
    "\tdf = processFile(fileOrFolderPath)\n",
    "\n",
    "print(\"Done :>\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv file   \n",
    "df.to_csv(f'../data/posts/r_{subreddit}_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [https://i.redditmedia.com/OABq21shxshcdZzNg8S...\n",
       "1    [https://i.redditmedia.com/r7XAKScrtgZ7rfEfcFe...\n",
       "2    [https://i.redditmedia.com/kDjA0yiANC1mgd2IUPM...\n",
       "3    [https://i.redditmedia.com/4-t6gwQX9gsviECzwEp...\n",
       "4    [https://i.redditmedia.com/MksE3bkT-HUF7mU3EWG...\n",
       "Name: ImagesUrls, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ImagesUrls'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts per Year:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timestamp\n",
       "2017     694\n",
       "2018    1583\n",
       "2019    3760\n",
       "2020    7584\n",
       "2021    9236\n",
       "2022    8764\n",
       "2023    7469\n",
       "2024    7345\n",
       "2025      24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_per_year = df.groupby(df['Timestamp'].dt.year).size()\n",
    "\n",
    "print(\"Posts per Year:\")\n",
    "posts_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df[\"NoReplies\"] >= 10]\n",
    "df = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Downloading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "successful_downloads = 0\n",
    "failed_downloads = 0\n",
    "\n",
    "\n",
    "\n",
    "def download_image(url, save_path, max_retries=2, timeout=20):\n",
    "    global successful_downloads, failed_downloads\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            logging.info(f\"Attempt {attempt + 1} to download {url}\")\n",
    "            response = requests.get(url, stream=True, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                logging.info(f\"Successfully downloaded {url} to {save_path}\")\n",
    "                successful_downloads += 1\n",
    "                return True\n",
    "            else:\n",
    "                logging.warning(f\"Failed to download {url} (status code: {response.status_code})\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            logging.warning(f\"Timeout occurred while downloading {url}.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Request exception for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error downloading {url}: {e}\")\n",
    "        \n",
    "        attempt += 1\n",
    "        if attempt < max_retries:\n",
    "            logging.info(f\"Retrying download for {url} (Attempt {attempt + 1}) after waiting for 5 seconds...\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "        else:\n",
    "            logging.error(f\"Skipping {url} after {max_retries} failed attempts.\")\n",
    "    failed_downloads += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_images:\n",
    "    total_images = sum(len(row['ImagesUrls']) for idx, row in df.iterrows())\n",
    "    with tqdm.tqdm(total=total_images, desc=\"Downloading images\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            \n",
    "            post_id = row['Id']\n",
    "            image_urls = row['ImagesUrls']\n",
    "            \n",
    "            for i, url in enumerate(image_urls):\n",
    "                filename = f\"{post_id}_row{idx}_img{i}.jpg\"\n",
    "                save_path = os.path.join(output_dir, filename)\n",
    "                if not os.path.exists(save_path):\n",
    "                    # Download the image\n",
    "                    download_image(url, save_path)\n",
    "                else:\n",
    "                    logging.info(f\"Skipping download for {url} as file already exists.\")\n",
    "                    successful_downloads += 1\n",
    "                # Update the progress bar and display counts of downloads\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\n",
    "                    \"Successful\": successful_downloads,\n",
    "                    \"Failed\": failed_downloads,\n",
    "                    \"Remaining\": total_images - (successful_downloads + failed_downloads)\n",
    "                })\n",
    "\n",
    "    print(f\"Successfully downloaded: {successful_downloads}\")\n",
    "    print(f\"Failed downloads: {failed_downloads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding captions to images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46459/46459 [00:01<00:00, 25485.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming 7843 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7843/7843 [00:00<00:00, 8241.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done renaming files with captions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt_variations(title: str) -> list[str]:\n",
    "    \"\"\"Generate different prompt variations for a fantasy/DnD map title\"\"\"\n",
    "    # Clean the title - remove common Reddit-specific patterns\n",
    "    title = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', title)  # Remove text in [] and ()\n",
    "    title = re.sub(r'OC|WIP|4K|HD', '', title, flags=re.IGNORECASE)  # Remove common tags\n",
    "    title = title.strip()\n",
    "    \n",
    "    # Base variations that describe the type of map\n",
    "    variations = [\n",
    "        f\"fantasy map of {title}\",\n",
    "        f\"DnD style map showing {title}\",\n",
    "        f\"hand-drawn fantasy map depicting {title}\",\n",
    "        f\"top-down view map of {title} in fantasy style\",\n",
    "    ]\n",
    "    \n",
    "    # Add variations based on common map styles\n",
    "    if any(word in title.lower() for word in ['city', 'town', 'village']):\n",
    "        variations.extend([\n",
    "            f\"medieval fantasy city map of {title}\",\n",
    "            f\"detailed town layout of {title} in fantasy style\",\n",
    "            f\"bird's eye view of fantasy settlement {title}\"\n",
    "        ])\n",
    "    \n",
    "    if any(word in title.lower() for word in ['dungeon', 'cave', 'lair']):\n",
    "        variations.extend([\n",
    "            f\"fantasy dungeon map of {title}\",\n",
    "            f\"DnD dungeon layout showing {title}\",\n",
    "            f\"top-down dungeon design of {title}\"\n",
    "        ])\n",
    "    \n",
    "    if any(word in title.lower() for word in ['region', 'realm', 'kingdom', 'world']):\n",
    "        variations.extend([\n",
    "            f\"fantasy world map of {title}\",\n",
    "            f\"detailed fantasy region map showing {title}\",\n",
    "            f\"hand-drawn fantasy realm of {title}\"\n",
    "        ])\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def sanitize_filename(text: str) -> str:\n",
    "    \"\"\"Convert text to a valid filename\"\"\"\n",
    "    # Remove invalid filename characters\n",
    "    valid_filename = re.sub(r'[<>:\"/\\\\|?*]', '', text)\n",
    "    # Replace spaces with underscores\n",
    "    valid_filename = valid_filename.replace(' ', '_')\n",
    "    # Limit length to avoid too long filenames\n",
    "    if len(valid_filename) > 200:\n",
    "        valid_filename = valid_filename[:200]\n",
    "    return valid_filename\n",
    "\n",
    "if add_captions:\n",
    "    print(\"Adding captions to images...\")\n",
    "\n",
    "    # Create a mapping of old filenames to new filenames\n",
    "    filename_mapping = {}\n",
    "\n",
    "    for idx, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "        post_id = row['Id']\n",
    "        title = row['Title']\n",
    "        \n",
    "        # Generate prompt variations\n",
    "        prompts = generate_prompt_variations(title)\n",
    "        prompt_text = \" || \".join(prompts)\n",
    "        \n",
    "        # For each image associated with this post\n",
    "        for i in range(len(row['ImagesUrls'])):\n",
    "            old_filename = f\"{post_id}_row{idx}_img{i}.jpg\"\n",
    "            new_filename = f\"{sanitize_filename(prompt_text)}__{post_id}_img{i}.jpg\"\n",
    "            \n",
    "            old_path = os.path.join(output_dir, old_filename)\n",
    "            new_path = os.path.join(output_dir, new_filename)\n",
    "            \n",
    "            if os.path.exists(old_path):\n",
    "                filename_mapping[old_path] = new_path\n",
    "\n",
    "    # Rename all files\n",
    "    print(f\"Renaming {len(filename_mapping)} files...\")\n",
    "    for old_path, new_path in tqdm.tqdm(filename_mapping.items()):\n",
    "        try:\n",
    "            os.rename(old_path, new_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {old_path}: {e}\")\n",
    "\n",
    "    print(\"Done renaming files with captions!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
